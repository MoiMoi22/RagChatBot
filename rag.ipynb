{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bf6c4f7",
   "metadata": {},
   "source": [
    "Install package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19be85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fastapi\n",
      "  Downloading fastapi-0.116.1-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting starlette<0.48.0,>=0.40.0 (from fastapi)\n",
      "  Downloading starlette-0.47.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in d:\\thuctap_totnghiep\\rag-server\\.myenv\\lib\\site-packages (from fastapi) (2.11.7)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\thuctap_totnghiep\\rag-server\\.myenv\\lib\\site-packages (from fastapi) (4.14.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in d:\\thuctap_totnghiep\\rag-server\\.myenv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in d:\\thuctap_totnghiep\\rag-server\\.myenv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in d:\\thuctap_totnghiep\\rag-server\\.myenv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.1)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in d:\\thuctap_totnghiep\\rag-server\\.myenv\\lib\\site-packages (from starlette<0.48.0,>=0.40.0->fastapi) (4.9.0)\n",
      "Requirement already satisfied: idna>=2.8 in d:\\thuctap_totnghiep\\rag-server\\.myenv\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in d:\\thuctap_totnghiep\\rag-server\\.myenv\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi) (1.3.1)\n",
      "Downloading fastapi-0.116.1-py3-none-any.whl (95 kB)\n",
      "Downloading starlette-0.47.2-py3-none-any.whl (72 kB)\n",
      "Installing collected packages: starlette, fastapi\n",
      "\n",
      "   ---------------------------------------- 0/2 [starlette]\n",
      "   ---------------------------------------- 0/2 [starlette]\n",
      "   ---------------------------------------- 0/2 [starlette]\n",
      "   ---------------------------------------- 0/2 [starlette]\n",
      "   -------------------- ------------------- 1/2 [fastapi]\n",
      "   -------------------- ------------------- 1/2 [fastapi]\n",
      "   -------------------- ------------------- 1/2 [fastapi]\n",
      "   -------------------- ------------------- 1/2 [fastapi]\n",
      "   -------------------- ------------------- 1/2 [fastapi]\n",
      "   ---------------------------------------- 2/2 [fastapi]\n",
      "\n",
      "Successfully installed fastapi-0.116.1 starlette-0.47.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: uvicorn in d:\\thuctap_totnghiep\\rag-server\\.myenv\\lib\\site-packages (0.35.0)\n",
      "Requirement already satisfied: click>=7.0 in d:\\thuctap_totnghiep\\rag-server\\.myenv\\lib\\site-packages (from uvicorn) (8.2.1)\n",
      "Requirement already satisfied: h11>=0.8 in d:\\thuctap_totnghiep\\rag-server\\.myenv\\lib\\site-packages (from uvicorn) (0.16.0)\n",
      "Requirement already satisfied: colorama in d:\\thuctap_totnghiep\\rag-server\\.myenv\\lib\\site-packages (from click>=7.0->uvicorn) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# %pip install llama-index-readers-file pymupdf\n",
    "# %pip install llama-index-vector-stores-chroma\n",
    "# %pip install llama-index\n",
    "# %pip install llama-index-embeddings-huggingface\n",
    "# %pip install chromadb\n",
    "# %pip install llama-index-llms-groq\n",
    "# %pip install fastapi\n",
    "# %pip install uvicorn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9318d1",
   "metadata": {},
   "source": [
    "Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a162e8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\THUCTAP_TOTNGHIEP\\rag-server\\.myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\THUCTAP_TOTNGHIEP\\rag-server\\.myenv\\Lib\\site-packages\\transformers\\utils\\hub.py:111: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# sentence transformers\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# llm api\n",
    "from llama_index.llms.groq import Groq\n",
    "# load data\n",
    "from pathlib import Path\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "\n",
    "# split documents\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# create node\n",
    "from llama_index.core.schema import TextNode\n",
    "\n",
    "# Vector store\n",
    "from IPython.display import Markdown, display\n",
    "import chromadb\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "#Load api key\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e38bc5",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d82ea60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tải xong!\n"
     ]
    }
   ],
   "source": [
    "# Tạo thư mục \"data\" nếu chưa tồn tại\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Tải file PDF\n",
    "url = \"https://arxiv.org/pdf/2307.09288.pdf\"\n",
    "headers = {\"User-Agent\": \"Chrome\"}\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "# Lưu file vào thư mục \"data\"\n",
    "with open(\"data/llama2.pdf\", \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "print(\"Tải xong!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e963144",
   "metadata": {},
   "source": [
    "Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33e94794",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = HuggingFaceEmbedding(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e55418",
   "metadata": {},
   "source": [
    "LLM QWEN API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5c0cc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()  # load biến môi trường từ file .env\n",
    "API_KEY = os.getenv(\"MY_API_KEY\")\n",
    "llm = Groq(model=\"qwen/qwen3-32b\", api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91035395",
   "metadata": {},
   "source": [
    "Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75a6e819",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyMuPDFReader()\n",
    "documents = loader.load(file_path=\"./data/llama2.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dec2cd",
   "metadata": {},
   "source": [
    "Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2212625",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_parser = SentenceSplitter(\n",
    "    chunk_size=1024,\n",
    "    # separator=\" \",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c120264",
   "metadata": {},
   "source": [
    "Text Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b174a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = []\n",
    "# maintain relationship with source doc index, to help inject doc metadata in (3)\n",
    "doc_idxs = []\n",
    "for doc_idx, doc in enumerate(documents):\n",
    "    cur_text_chunks = text_parser.split_text(doc.text)\n",
    "    text_chunks.extend(cur_text_chunks)\n",
    "    doc_idxs.extend([doc_idx] * len(cur_text_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d8a070",
   "metadata": {},
   "source": [
    "Create Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01f27ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = []\n",
    "for idx, text_chunk in enumerate(text_chunks):\n",
    "    node = TextNode(\n",
    "        text=text_chunk,\n",
    "    )\n",
    "    src_doc = documents[doc_idxs[idx]]\n",
    "    node.metadata = src_doc.metadata\n",
    "    nodes.append(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbe6f02",
   "metadata": {},
   "source": [
    "Save DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eae72e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "chroma_collection = db.get_or_create_collection(\"Test\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex(nodes,\n",
    "                         storage_context=storage_context,\n",
    "                         embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a44c05",
   "metadata": {},
   "source": [
    "Load DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1687f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from disk\n",
    "db2 = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "chroma_collection = db2.get_or_create_collection(\"Test\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_vector_store(vector_store, embed_model=embed_model, storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b137db5",
   "metadata": {},
   "source": [
    "Query + Retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c429a3b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b><think>\n",
       "Okay, let's tackle this query. The user is asking, \"What is llama2?\" I need to use the provided context information to form an answer.\n",
       "\n",
       "First, I'll look through the context. There's a model card mentioned in the first part. It says Llama 2 comes in different parameter sizes: 7B, 13B, and 70B. It's developed by Meta AI and has both pretrained and fine-tuned versions. The input is text-only, and the output is text-only. The architecture is an autoregressive transformer with SFT and RLHF for alignment. Training dates are between January and July 2023. The license is a custom commercial one, and there's info on intended use for commercial and research in English.\n",
       "\n",
       "The second part of the context has a figure showing the evolution of Llama 2-Chat through different versions, using RLHF and SFT. It mentions evaluations against ChatGPT and other models, with Llama 2-Chat performing well in helpfulness and harmlessness. Human evaluations also show it outperforms open-source models.\n",
       "\n",
       "Putting this together, the answer should mention the different versions (pretrained vs. fine-tuned), parameter sizes, training methods (SFT, RLHF), intended uses, and performance highlights. Also, note the ethical considerations and licensing. Avoid mentioning the context directly, just the facts from it. Make sure to structure it clearly and concisely.\n",
       "</think>\n",
       "\n",
       "Llama 2 is a series of large language models developed by Meta AI, available in three parameter sizes: 7B, 13B, and 70B. It includes both **pretrained** and **fine-tuned** variants, with the latter optimized for chat-like interactions using techniques like supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF). The models are designed for text-only input and output, leveraging an optimized transformer architecture. \n",
       "\n",
       "Key features include:\n",
       "- **Training**: Pretrained on 2 trillion tokens (data cutoff September 2022) and fine-tuned with over 1 million human-annotated examples.\n",
       "- **Performance**: Evaluations show Llama 2-Chat variants outperform open-source and closed-source competitors in helpfulness and safety metrics, with versions like Llama 2-Chat 7B and 34B achieving high win rates in comparisons.\n",
       "- **Use Cases**: Intended for commercial and research applications in English, with ethical guidelines emphasizing safety testing and adherence to licensing terms.\n",
       "- **Availability**: Released under a custom commercial license, with access details provided by Meta. \n",
       "\n",
       "The models were trained between January and July 2023, with a focus on aligning outputs to human preferences for safety and helpfulness. Meta also highlights efforts to offset the carbon footprint of training.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Query Data\n",
    "query_engine = index.as_query_engine(llm)\n",
    "response = query_engine.query(\"What is llama2?\")\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
